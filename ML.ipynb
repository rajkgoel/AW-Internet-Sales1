{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNELUUmu+7pHYXeSaNmNBp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajkgoel/AW-Internet-Sales1/blob/master/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning**\n",
        "**Phase 1: Foundations**\n",
        "1. Linear Regression (core + prediction)\n",
        "2. Regression error metrics (MAE, MSE, RMSE)\n",
        "3. Feature Scaling (Normalization vs Standardization)\n",
        "\n",
        "\n",
        "**Phase 2: Model Behavior**\n",
        "\n",
        "4. Generalization & Overfitting\n",
        "5. Polynomial Regression\n",
        "6. Multicollinearity & VIF\n",
        "\n",
        "\n",
        "**Phase 3: Regularization**\n",
        "\n",
        "7. Ridge Regression\n",
        "8. Lasso Regression\n",
        "\n",
        "\n",
        "**Phase 4: Classification**\n",
        "\n",
        "9. Logistic Regression\n",
        "10. Sigmoid & Decision Boundary\n",
        "11. Log-Loss"
      ],
      "metadata": {
        "id": "pBBQ5zNAFWmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Linear Regression"
      ],
      "metadata": {
        "id": "6J7zGnKimv5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression predicts a continuous value using a weighted sum of features and an intercept.\n",
        "\n",
        "$ y^â€‹= wáµ€*x + b $\n"
      ],
      "metadata": {
        "id": "G5F2qf74nvRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aolb9FBcjRcM",
        "outputId": "ad61b87a-2263-4b5f-a34b-74c27d7df91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient (price increase psft): 0.060000000000000005\n",
            "Intercept (base price when area = 0): 0.0\n",
            "Predicted price for 1800 sq ft (plugs into wx+b): 108.00000000000001\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Data\n",
        "X = np.array([[500], [1000], [1500], [2000]])\n",
        "y = np.array([30, 60, 90, 120])\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Learned parameters\n",
        "print(\"Coefficient (price increase psft):\", model.coef_[0])\n",
        "print(\"Intercept (base price when area = 0):\", model.intercept_)\n",
        "\n",
        "# Prediction\n",
        "print(\"Predicted price for 1800 sq ft (plugs into wx+b):\", model.predict([[1800]])[0])\n",
        "print(\"A feature with zero weight has no influence on the modelâ€™s prediction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - MAE, MSE, RMSE"
      ],
      "metadata": {
        "id": "ym-ns0jg968x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These measure how wrong predictions are.\n",
        "\n",
        "Metric\tMeaning\n",
        " - MAE:\tAverage absolute error\n",
        " - MSE:\tAverage squared error\n",
        " - RMSE:\tSquare root of MSE\n",
        "\n",
        "**Actual vs Predicted house prices (â‚¹ lakh)**\n",
        "\n",
        "| Actual |\tPredicted |\n",
        "| --- | --- |\n",
        "| 100 |\t    90 |\n",
        "| 150 |     160 |\n",
        "| 200\t|    210 |\n",
        "\n",
        "Errors - [-10, 10, 10]\n",
        "\n",
        "- MAE = $(|-10| + |10| + |10|)/3$ = 30/3 = 10\n",
        "- MSE = $(100+100+100)/3$ = 100\n",
        "- RMSE = $\\sqrt{100}$ = 10\n",
        "\n",
        "RMSE is more sensitive to outliers because it squares the errors, giving larger penalties to big mistakes. RMSE is preferred because it penalizes large errors more heavily, making it easier to detect and discourage serious prediction mistakes."
      ],
      "metadata": {
        "id": "2UdveTAvn33p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "y_true = np.array([100, 150, 200])\n",
        "y_pred = np.array([90, 160, 210])\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYx-Qrx__t_o",
        "outputId": "11d754cc-dcdd-430a-be08-3daba7210b3c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 10.0\n",
            "MSE: 100.0\n",
            "RMSE: 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Feature Scaling\n"
      ],
      "metadata": {
        "id": "sIiCG2Oro6vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling brings all features to a comparable range so that learning is stable, fair, and fast.\n",
        "It is critical for:\n",
        " - Gradient Descent\n",
        " - Ridge / Lasso\n",
        " - Logistic Regression\n",
        " - Distance-based models\n",
        "\n",
        "**Why scaling matters?**\n",
        "\n",
        "Predict house price using:\n",
        " - Area (sq ft) â†’ 500 to 5000\n",
        " - Age of house (years) â†’ 1 to 30\n",
        "\n",
        "Without scaling Area dominates just because numbers are bigger, Age effect is ignored or unstable\n",
        "\n",
        "**Two common scaling methods**\n",
        "1. Normalization (Min-Max)\n",
        "\n",
        "$ x' = (x - x_min)/(x_max - x_min) $\n",
        "\n",
        "    Range -> [0,1]\n",
        "\n",
        "2. Standardization (Z-score)\n",
        "\n",
        "$ x' = (x - Î¼)/Ïƒ $\n",
        "\n",
        "    Mean -> 0, Std -> 1\n",
        "\n",
        "**Impact Summary**\n",
        "\n",
        " - Prevents one feature from overpowering others\n",
        " - Makes weights stable\n",
        " - Essential for regularization\n",
        " - Does NOT change relationships, only scale"
      ],
      "metadata": {
        "id": "htHiAuY0q5y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "X = np.array([\n",
        "    [500, 1],\n",
        "    [1500, 10],\n",
        "    [3000, 20],\n",
        "    [5000, 30]\n",
        "])\n",
        "\n",
        "# Normalization\n",
        "minmax = MinMaxScaler()\n",
        "X_norm = minmax.fit_transform(X)\n",
        "\n",
        "# Standardization\n",
        "standard = StandardScaler()\n",
        "X_std = standard.fit_transform(X)\n",
        "\n",
        "print(\"Normalized:\\n\", X_norm)\n",
        "print(\"\\nStandardized:\\n\", X_std)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNhTaITUxGGr",
        "outputId": "b7f9d7b2-3aae-4522-e104-312fcae2ae44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized:\n",
            " [[0.         0.        ]\n",
            " [0.22222222 0.31034483]\n",
            " [0.55555556 0.65517241]\n",
            " [1.         1.        ]]\n",
            "\n",
            "Standardized:\n",
            " [[-1.17953565 -1.31355934]\n",
            " [-0.58976782 -0.48394291]\n",
            " [ 0.29488391  0.43785311]\n",
            " [ 1.47441956  1.35964914]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - Generalization & Overfitting"
      ],
      "metadata": {
        "id": "RoDJ8ljALtNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ï¸âƒ£ Concept\n",
        "Generalization is how well a model performs on unseen (test) data.\n",
        "\n",
        " - Good model â†’ similar train error & test error\n",
        " - Overfitting â†’ very low train error, high test error\n",
        " - Underfitting â†’ high error on both\n",
        "\n",
        "2ï¸âƒ£ Real-world intuition ðŸŒ\n",
        "Example: Exam preparation\n",
        "\n",
        "ðŸ‘‰ Overfitting - You memorize answers â†’ perfect on practice questions â†’ fail new questions\n",
        "\n",
        "ðŸ‘‰ Generalization - You understand concepts â†’ good on new questions\n",
        "\n",
        "Same idea in ML.\n",
        "\n",
        "3ï¸âƒ£ How overfitting happens in ML\n",
        "\n",
        "Common causes:\n",
        "- Model too complex (high-degree polynomial)\n",
        "- Too many features\n",
        "- Too little data\n",
        "- Noise in training data\n",
        "\n",
        "Overfitting occurs when a model learns noise instead of patterns, performing well on training data but poorly on unseen data."
      ],
      "metadata": {
        "id": "rF9B-sKXrCqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Data\n",
        "X_input = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "print(\"X_input: \", X_input)\n",
        "y_output = np.array([1.2, 1.9, 3.0, 3.9, 5.1])\n",
        "\n",
        "# Linear model\n",
        "lin = LinearRegression()\n",
        "lin.fit(X_input, y_output)\n",
        "y_pred_lin = lin.predict(X_input)\n",
        "print(f\"Linear Equation: y = {model.coef_[0]:.2f} * x + {model.intercept_:.2f}\")\n",
        "print(\"y_pred_lin: \", y_pred_lin)\n",
        "\n",
        "# Polynomial model (degree 4)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X_input)\n",
        "\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_poly, y_output)\n",
        "y_pred_poly = poly_model.predict(X_poly)\n",
        "\n",
        "print(\"Linear MSE:\", mean_squared_error(y, y_pred_lin))\n",
        "print(\"Polynomial MSE:\", mean_squared_error(y, y_pred_poly))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6FbBuAsNm1x",
        "outputId": "f5411def-c667-4112-b018-9e4c42e3c37d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_input:  [[1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]]\n",
            "Linear Equation: y = 0.06 * x + 0.00\n",
            "y_pred_lin:  [1.06 2.04 3.02 4.   4.98]\n",
            "Linear MSE: 0.012799999999999978\n",
            "Polynomial MSE: 9.61907405542556e-28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5 - Polynomial Regression"
      ],
      "metadata": {
        "id": "YjIyeok_0ECV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Regression is still Linear Regression, but:\n",
        "- We transform features by adding powers of ð‘¥\n",
        "- The model stays linear in weights\n",
        "\n",
        "$y= w_0 + w_1x^1 + w_2x^2 + w_3x^3  $\n",
        "\n",
        "So:\n",
        "- Degree = highest power of x\n",
        "- Higher degree â†’ more flexible curve\n",
        "\n",
        "ðŸ”¹ Why we need it (real-world intuition)\n",
        "\n",
        "**Linear regression assumes:**\n",
        "\n",
        "Change in y is proportional to change in x, but many real-world problems are non-linear.\n",
        "\n",
        "**Real-world examples**\n",
        "| Problem |\tWhy polynomial helps |\n",
        "|---|---|\n",
        "| Car speed vs braking distance |\tDistance grows faster at high speed |\n",
        "| Ad spend vs sales |\tDiminishing returns |\n",
        "| Age vs insurance risk |\tRisk rises faster after certain age |\n",
        "| CPU load vs response time |\tExplodes after threshold |\n",
        "\n",
        "ðŸ”¹ Risk (important!)\n",
        "\n",
        "Higher degree:\n",
        "\n",
        "Fits training data very well -\n",
        "- Can overfit\n",
        "- Causes multicollinearity\n",
        "- Coefficients become unstable\n",
        "\n",
        "High-degree polynomial regression with limited data increases overfitting and introduces severe multicollinearity due to highly correlated polynomial terms.\n",
        "\n",
        "This is why:\n",
        "ðŸ‘‰ feature scaling + regularization matter."
      ],
      "metadata": {
        "id": "RXES4thhrO-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1, 4, 9, 16, 25])  # y = x^2\n",
        "\n",
        "# Polynomial Regression (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# Prediction\n",
        "x_new = np.array([[6]])\n",
        "x_new_poly = poly.transform(x_new)\n",
        "print(model.predict(x_new_poly))\n",
        "\n",
        "print(\"\\nIf you increase degree from 2 â†’ 10 - \\nWith a higher degree and limited data, the model overfits, and the risk of multicollinearity increases significantly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krXr4wl9QWHw",
        "outputId": "c3ae064c-4730-4628-d904-2ea9fdfcb458"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [ 0.00000000e+00 -8.77076189e-15  1.00000000e+00]\n",
            "Intercept: 7.105427357601002e-15\n",
            "[36.]\n",
            "\n",
            "If you increase degree from 2 â†’ 10 - \n",
            "With a higher degree and limited data, the model overfits, and the risk of multicollinearity increases significantly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6 - VIF"
      ],
      "metadata": {
        "id": "5MCu2CxSTxpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIF (Variance Inflation Factor) tells us how much a feature is explained by other features.\n",
        "- Low VIF â†’ feature is independent\n",
        "- High VIF â†’ feature is redundant (multicollinearity)\n",
        "\n",
        "$ VIF_i = 1 / (1 - R^2_i) $\n",
        "\n",
        "$ R^2_i = $ how well this feature can be predicted using other features\n",
        "\n",
        "ðŸ‘‰ If others can predict it well â†’ itâ€™s redundant.\n",
        "\n",
        "---\n",
        "\n",
        "**VIF interpretation**\n",
        "| VIF value\t| Meaning |\n",
        "| --- | --- |\n",
        "| 1 |\tNo multicollinearity |\n",
        "| 1â€“5 | Acceptable |\n",
        "| 5â€“10 |\tHigh |\n",
        "| >10 | \tSevere multicollinearity |\n",
        "| âˆž |\tPerfect multicollinearity |\n",
        "\n",
        "---\n",
        "**Real-world intuition**\n",
        "\n",
        "Suppose:\n",
        "- Feature 1: House area (sq ft)\n",
        "- Feature 2: House area (sq meters)\n",
        "\n",
        "They say the same thing â†’ VIF â†’ âˆž\n",
        "\n",
        "Model gets confused: â€œWho should I give credit to?â€\n",
        "\n",
        "This proves: Polynomial Regression â†’ Multicollinearity\n"
      ],
      "metadata": {
        "id": "AKyIzikdrZWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "\n",
        "X = pd.DataFrame({ \"x\": [1, 2, 3, 4, 5] })\n",
        "X[\"x2\"] = X[\"x\"] ** 2\n",
        "X[\"x3\"] = X[\"x\"] ** 3\n",
        "\n",
        "print(X)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [\n",
        "    variance_inflation_factor(X.values, i)\n",
        "    for i in range(X.shape[1])\n",
        "]\n",
        "\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fziKgHf3VwoS",
        "outputId": "8e682ffa-4c2c-485b-ee13-eb236bab8108"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   x  x2   x3\n",
            "0  1   1    1\n",
            "1  2   4    8\n",
            "2  3   9   27\n",
            "3  4  16   64\n",
            "4  5  25  125\n",
            "  feature         VIF\n",
            "0       x  113.537157\n",
            "1      x2  681.803571\n",
            "2      x3  270.801768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This proves: Polynomial Regression â†’ Multicollinearity\n",
        "\n",
        "Why this is dangerous\n",
        "- Coefficients become unstable\n",
        "- Signs (+/âˆ’) may flip\n",
        "- Model interpretation breaks\n",
        "- Small data change â†’ big weight change\n",
        "\n",
        "**How this connects to Ridge & Lasso (preview)**\n",
        "- Ridge â†’ reduces coefficient explosion\n",
        "- Lasso â†’ may drop redundant polynomial terms\n",
        "- Feature scaling â†’ required before regularization\n",
        "\n",
        "VIF measures how much the variance of a coefficient is inflated due to multicollinearity; high VIF indicates redundant features.\n",
        "\n",
        "In polynomial regression, higher-order terms are highly correlated, which leads to multicollinearity and causes coefficient instability, making some coefficients redundant and unreliable.\n",
        "\n",
        "Polynomial regression often introduces multicollinearity, inflating VIF values and making coefficient estimates unstable unless regularization is applied."
      ],
      "metadata": {
        "id": "iSsZ6t4wWubP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7 - Ridge Regression"
      ],
      "metadata": {
        "id": "Mr6BFUYBrpdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**:\n",
        "\n",
        "Polynomial features â†’ multicollinearity â†’ unstable coefficients\n",
        "\n",
        "**Ridge fixes this by:**\n",
        "\n",
        "Penalizing large coefficients\n",
        "\n",
        "Ridge minimizes: $ Loss = MSE + Î± âˆ‘ w^2_i $\n",
        "\n",
        "Where:\n",
        " - MSE â†’ fit the data\n",
        " - ð›¼ â†’ penalty strength\n",
        " - $ w^2_i $ â†’ discourages large weights\n",
        "\n",
        "ðŸ‘‰ Coefficients are shrunk, but never exactly zero.\n",
        "\n",
        "**Why Ridge works well with Polynomial Regression**\n",
        "\n",
        "Polynomial terms are correlated:\n",
        "\n",
        "x, xÂ², xÂ³ â†’ multicollinearity\n",
        "\n",
        "**Ridge:**\n",
        "- Keeps all features\n",
        "- Prevents coefficient explosion\n",
        "- Improves generalization\n"
      ],
      "metadata": {
        "id": "0z-T1bWhr2HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "poly = PolynomialFeatures(degree=5)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_poly, y)\n",
        "\n",
        "print(\"Coefficients:\", ridge.coef_)\n",
        "\n",
        "# Print the polynomial equation\n",
        "feature_names = poly.get_feature_names_out([\"x\"])\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "\n",
        "equation = f\"y = {intercept:.3f}\"\n",
        "\n",
        "for coef, name in zip(coefficients[1:], feature_names[1:]):\n",
        "    equation += f\" + ({coef:.3f} * {name})\"\n",
        "\n",
        "print(\"Equation: \", equation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ3NFtMDuGvT",
        "outputId": "3c6ba879-40c5-4e19-fb5f-6c9d1115867d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [ 0.          0.12759014  0.30241456  0.3763165  -0.07279512  0.00474731]\n",
            "Equation:  y = 0.000 + (-0.000 * x) + (1.000 * x^2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Effect** of Î± (alpha)\n",
        "\n",
        "| Alpha | Effect |\n",
        "| --- | --- |\n",
        "| 0 |\tSame as Linear Regression |\n",
        "| Small |\tMild regularization |\n",
        "| Large |\tStrong shrinkage |\n",
        "| Very large |\tUnderfitting |\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ“Œ Alpha is chosen using cross-validation.\n",
        "\n",
        "**Key differences (Ridge vs Linear)**\n",
        "\n",
        "| Aspect |\tLinear |\tRidge |\n",
        "| --- | --- | --- |\n",
        "| Multicollinearity\t| âŒ unstable |\tâœ… stable |\n",
        "| Coefficient size\t| Can explode |\tShrinks |\n",
        "| Feature removal |\tâŒ no |\tâŒ no |\n",
        "| Overfitting |\tâŒ |\tâœ… reduced |\n",
        "\n",
        "Ridge Regression adds an L2 penalty to the loss function to reduce coefficient variance and handle multicollinearity.\n",
        "\n",
        "A very large Ridge penalty forces coefficients toward zero, oversimplifying the model and causing underfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "i5shv9kVyPYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 8 - Lasso Regression"
      ],
      "metadata": {
        "id": "LQssvkJw4jtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso = Linear Regression + L1 regularization\n",
        "\n",
        "It minimizes:\n",
        "\n",
        "$ Loss = MSE + ð›¼ âˆ‘ | w_i | $\n",
        "\n",
        "**Key difference from Ridge**\n",
        "| Aspect |\tRidge |\tLasso |\n",
        "| --- | --- | --- |\n",
        "| Penalty | $ w^2 (L2) $ | ( |\n",
        "| Coefficients |\tShrinks\t| Shrinks to zero |\n",
        "| Feature selection | âŒ No |\tâœ… Yes |\n",
        "| Multicollinearity |\tHandles |\tDrops redundant features |\n",
        "\n",
        "ðŸ‘‰ Lasso can remove features completely.\n",
        "\n",
        "**Intuition**\n",
        "\n",
        "Imagine squeezing coefficients with a box ðŸ“¦:\n",
        "\n",
        "- Ridge â†’ smooth squeeze (never hits zero)\n",
        "- Lasso â†’ sharp corners â†’ coefficients hit zero\n",
        "\n",
        "This is why Lasso produces sparse models.\n",
        "\n",
        "**Why Lasso is useful**\n",
        "\n",
        "- High-dimensional data\n",
        "- Polynomial features\n",
        "- When you want interpretability\n",
        "- Automatic feature selection\n",
        "\n",
        "Eg - x, xÂ², xÂ³, xâ´, Lasso may keep only xÂ²\n"
      ],
      "metadata": {
        "id": "uNtoaaZV4tik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Lasso\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "poly = PolynomialFeatures(degree=5)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
        "lasso.fit(X_poly, y)\n",
        "\n",
        "print(\"Coefficients:\", lasso.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFL1vfqDFHQ8",
        "outputId": "b915ec1c-a737-454c-f196-29d36b4eca91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [ 0.          0.          0.56938603  0.14342716 -0.00267198 -0.00193316]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use what?**\n",
        "| Situation                         | Use   |\n",
        "|--- | ---|\n",
        "| Multicollinearity                | Ridge |\n",
        "| Feature selection                | Lasso |\n",
        "| Interpretability                 | Lasso |\n",
        "| All features important           | Ridge |\n",
        "| Many features, few samples       | Lasso |\n",
        "\n",
        "Lasso regression uses L1 regularization to shrink coefficients and set some to zero, performing feature selection.\n",
        "\n",
        "Feature scaling mandatory before Lasso because it ensures that no single feature dominates the regularization penalty, allowing Lasso to treat all features fairly. Without feature scaling, Lassoâ€™s penalty becomes biased toward features with smaller numerical scales."
      ],
      "metadata": {
        "id": "i3sjGhGNMrDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 9 - Logistic Regression"
      ],
      "metadata": {
        "id": "WLJ2u72LYpx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we need Logistic Regression?**\n",
        "\n",
        "Linear Regression:\n",
        "- Outputs any real number (âˆ’âˆž to +âˆž)\n",
        "- âŒ Not suitable for classification\n",
        "\n",
        "But classification needs:\n",
        "- Yes / No\n",
        "- 0 / 1\n",
        "- Probability\n",
        "\n",
        "ðŸ‘‰ Logistic Regression is used when output is categorical, especially binary.\n",
        "\n",
        "**Logistic Regression does two steps:**\n",
        "\n",
        "> Step 1: Linear combination (same as Linear Regression)\n",
        "\n",
        ">> $ z = w_1x_1 + w_2x_2 + ... + w_0 $\n",
        "\n",
        "> Step 2: Apply Sigmoid function\n",
        "\n",
        ">> $ \\hat{y} = Ïƒ(z) = 1/(1 + e^z) $\n",
        "\n",
        "This converts any number into:\n",
        "\n",
        "$ 0 <= \\hat{y} <= 1 $\n",
        "\n",
        "ðŸ‘‰ Output = probability of class 1\n",
        "\n",
        "---\n",
        "\n",
        "**What does the Sigmoid do?**\n",
        "\n",
        "|z value | Sigmoid output |\n",
        "| --------------| --- |\n",
        "| Very negative |\tâ‰ˆ 0 |\n",
        "| 0 |\t0.5 |\n",
        "| Very positive\t| â‰ˆ 1 |\n",
        "\n",
        "So:\n",
        "- Large positive z â†’ Yes\n",
        "- Large negative z â†’ No\n",
        "\n",
        "**Decision boundary -**\n",
        "We classify using a threshold (usually 0.5):\n",
        "\n",
        "> $ \\hat{y} >= 0.5 â‡’ Class 1 $\n",
        "\n",
        "> $ \\hat{y} < 0.5 â‡’ Class 0 $\n",
        "\n",
        "This corresponds to: z = 0\n",
        "\n",
        "ðŸ‘‰ Decision boundary = linear equation\n",
        "\n",
        "---\n",
        "\n",
        "**Log-Loss**\n",
        "\n",
        "For one data point:\n",
        "\n",
        "> $ Loss=âˆ’[ ylog(p) + (1âˆ’y)log(1âˆ’p) ] $\n",
        "\n",
        "Where:\n",
        "- Correct & confident â†’ low loss\n",
        "- Wrong & confident â†’ huge loss\n",
        "\n",
        "ðŸ‘‰ This is why Logistic Regression is probabilistically sound."
      ],
      "metadata": {
        "id": "yCrJ1zUTZDhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(model.predict([[2.5]]))        # class\n",
        "print(model.predict_proba([[2.5]]))  # probability - Class 0, Class 1\n",
        "# predict_proba() returns class probabilities, while predict() applies a threshold to return the final class label.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d6TTCOa6AWl",
        "outputId": "6128d33e-4cc4-4783-8ee7-c376791f9e16"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[[0.49999774 0.50000226]]\n"
          ]
        }
      ]
    }
  ]
}